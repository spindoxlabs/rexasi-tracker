"use strict";(self.webpackChunkrexasi_tracker=self.webpackChunkrexasi_tracker||[]).push([[842],{6159:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>n,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"add-detector/available","title":"Available detectors","description":"With the Rexasi Tracker come the following, ready to use, people detectors.","source":"@site/docs/add-detector/available.md","sourceDirName":"add-detector","slug":"/add-detector/available","permalink":"/rexasi-tracker/docs/add-detector/available","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/add-detector/available.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"documentationSidebar","previous":{"title":"Add a detector","permalink":"/rexasi-tracker/docs/category/add-a-detector"},"next":{"title":"Use a custom detector","permalink":"/rexasi-tracker/docs/add-detector/custom"}}');var s=r(4848),i=r(8453);const n={sidebar_position:1},a="Available detectors",c={},d=[{value:"LiDAR detector",id:"lidar-detector",level:2},{value:"RGBD detector",id:"rgbd-detector",level:2}];function l(e){const t={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"available-detectors",children:"Available detectors"})}),"\n",(0,s.jsx)(t.p,{children:"With the Rexasi Tracker come the following, ready to use, people detectors."}),"\n",(0,s.jsx)(t.h2,{id:"lidar-detector",children:"LiDAR detector"}),"\n",(0,s.jsxs)(t.p,{children:["The LiDAR detector uses the ",(0,s.jsx)(t.a,{href:"https://arxiv.org/abs/2004.14079",children:"DR-SPAAM"})," model to detect people from 2D LiDAR data.\nThe implementation is based on the ",(0,s.jsx)(t.a,{href:"https://github.com/VisualComputingInstitute/2D_lidar_person_detection",children:"2D_lidar_person_detection"})," library. ",(0,s.jsx)(t.br,{}),"\n","Code and configuration under ",(0,s.jsx)(t.a,{href:"https://github.com/spindoxlabs/rexasi-tracker/tree/main/detectors/lidar",children:"LiDAR folder"}),"."]}),"\n",(0,s.jsx)(t.h2,{id:"rgbd-detector",children:"RGBD detector"}),"\n",(0,s.jsxs)(t.p,{children:["The RGBD detector uses both color and depth images to detect the position of people from pose keypoints. ",(0,s.jsx)(t.br,{}),"\n","Code and configuration under ",(0,s.jsx)(t.a,{href:"https://github.com/spindoxlabs/rexasi-tracker/tree/main/detectors/rgbd",children:"RGBD folder"}),". ",(0,s.jsx)(t.br,{}),"\n","This implementation consists of two ROS2 nodes:"]}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Pose estimation"}),": for each color frame given as input, this node apply the ",(0,s.jsx)(t.a,{href:"https://docs.ultralytics.com/it/models/yolov9/",children:"YOLOv9 Pose"})," model to detect the body joints of each people in the image. The output is a ",(0,s.jsx)(t.a,{href:"https://github.com/spindoxlabs/rexasi-tracker/blob/main/detectors/rgbd/ros/rgbd_detector_msgs/msg/Persons.msg",children:"Persons"})," message, for each person a ",(0,s.jsx)(t.a,{href:"https://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/PoseArray.html",children:"PoseArray"})," message contains the list of keypoints with their confidence."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Detector"}),": for each person, this node computes the projection of the keypoints from pixel coordinates to 3D world coordinates, then estimates and outputs the center of the person. The projection is done by extracting the keypoints distance using the depth image data along with the camera intrinsics. The library used to compute the projection is the ",(0,s.jsx)(t.a,{href:"https://github.com/IntelRealSense/librealsense",children:"librealsense2"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:["The RGBD detector considers the camera position centered in the ",(0,s.jsx)(t.code,{children:"world"})," frame. If the camera publishes its own static TF, you can use it by changing the ",(0,s.jsx)(t.code,{children:"frame_id"})," and ",(0,s.jsx)(t.code,{children:"optical_frame_id"})," in the launch configuration (",(0,s.jsx)(t.a,{href:"https://github.com/spindoxlabs/rexasi-tracker/blob/main/detectors/rgbd/ros/rgbd_detector/launch/launch.py",children:"RGBD launch file"}),"), so the detections are transformed with respect to the camera position."]})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,t,r)=>{r.d(t,{R:()=>n,x:()=>a});var o=r(6540);const s={},i=o.createContext(s);function n(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:n(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);